
### 4.2.1 Definition invertierbarer Matrizen
Sei $R$ ein kommutativer [[3.1 Ringe & Ringhomomorphismen|Ring]]. 
Die [[3.1 Ringe & Ringhomomorphismen#3.1.10 Einheiten, Einheitengruppe|Einheitengruppe]] des Rings $R^{p\times p}$ bezeichnet man mit $GL_p(R)$:
	$GL_p(R) = \{A\in R^{p\times p} | \exists B \in R^{p\times p}: AB = BA = I_p\}$
Die Matrizen in $GL_p(R)$ heißen __invertierbare__ Matrizen.

### 4.2.3 Elementarmatrizen
Sei $R$ ein kommutativer [[3.1 Ringe & Ringhomomorphismen|Ring]]. 
Die _Elementarmatrix_ $E_{i_j}$ ist gegeben durch:
$$
E_{i_j}(j, k) := 
\begin{cases}
	1 &\text{falls } i = k \text{ und } j=l\\
	0 &\text{sonst}
\end{cases}
$$
Die _Elementarmatrix_ hat also __genau eine 1__ an der stelle $(i, j)$, und sonst nur Nullen.

Für $E_{i_j} \in R^{p\times q}, E_{k_l} \in R^{q\times s}$ gilt:
$$
	E_{i_j} \cdot E_{k_l} = 
\begin{cases}
	E_{i_j} &\text{falls } j=k\\
	0 &\text{sonst}
\end{cases}
$$
__Für Elementarmatrizen mit nur einer Spalte__:
	$e_j := E_{i_1} \in R^q$ gilt:
	$A \cdot e_j = \sum_{i=1}^p a_{i_j} e_i \in R^p$


### 4.2.5 Additionsmatrizen
Sei $A \in R^{p\times p}$ eine Matrix.
Sei  $\alpha \in R$
Für $i \neq j$ und $1 \leq i, j \leq p$ ist die __Additionsmatrix__ definiert durch:
	$A_{i_j}(\alpha) := I_p + \alpha E_{i_j}$
Sie ist die [[4.1 Lineare Gleichungssysteme#4.1.12 Einheitsmatrix|Einheitsmatrix]] mit $\alpha$ an Stelle $(i, j)$.
__Additionsmatrizen__ sind _invertierbar_.

Für das Produkt einer Matrix $M$ mit $A_{i_j}$ gilt:
$A_{i_j}(\alpha) \cdot M$ entsteht indem man zur $i$-ten Zeile das $a$-fache der $j$-ten Zeile addiert.

### 4.2.6 Vertauschungsmatrix
Für $1 \leq 1, j \leq p$ sei die __Vertauschungsmatrix__ $V_{i_j} \in R^{p\times p}$ definiert durch:
	$V_{i_j} := I_p - E_{i_i} - E_{j_j} + E_{i_j} + E_{j_i}$
Die Matrix $V_{i_j}$ entsteht durch das Vertauschen der __Einsen__ an den Stellen $(i, i)$ und $(j, j)$ mit __Einsen__ an den Stellen $(j, i)$ und $(i, j)$.

Multipliziert man $V_{i_j}$ mit einer Matrix $M$, vertauscht sich die $i$-te und $j$-te Zeile.
Außerdem gilt:
	$V_{i_j} \cdot V_{i_j} = I_p$

### 4.2.7 Diagonalmatrizen
Für Elemente $\alpha_1, \alpha_2, \dots, \alpha_p$ ist die __Diagonalmatrix__ $diag(\alpha_1, \dots, \alpha_p)$ definiert durch:
	$diag(\alpha_1, \dots, \alpha_p) := \sum_{i=1}^p \alpha_i E_{i_i} \in R^{p\times p}$
Multipliziert man $diag(\alpha_1, \dots, \alpha_p)$ mit einer Matrix $M$ gilt:
	$diag(\alpha_1, \dots, \alpha_p) \cdot M = \sum_{i, j} \alpha_i m_{i_j} E_{i_j}$
Es wird für alle $i$ die $i$-te Zeile mit $\alpha_i$ multipliziert.
Wenn für alle $\alpha \in R$ gilt:
	$diag(\alpha_1, \dots, \alpha_p)$ ist invertierbar.

### 4.2.9 Determinante
Sei $A$ eine $2\times2$-Matrix:
	$A =\begin{pmatrix}	a & b \\	c & d\end{pmatrix}$
$A$ ist genau dann _invertierbar_, wenn 
	$det(A) := ad - bc$ 
eine [[3.1 Ringe & Ringhomomorphismen#3.1.10 Einheiten, Einheitengruppe|Einheit]] auf R ist ($det(a) \neq 0$ gilt)
Die _invertierte_ Matrix $E$ erhält man durch:
	$E := (det(A))^{-1} \cdot \begin{pmatrix} d & -b \\ -c & a \end{pmatrix}$
es gilt:
	$AE = EA = I_2$
$det(A)$ wird __Determinante__ genannt.


